setwd("/Users/taomo/Documents/workspace/MLProject/MachineLearningProject/Rcode")
data = read.table('data.txt')
library(randomForest)
fit <- randomForest(V9 ~ .,   data=data)
print(fit) # view results
importance(fit) # importance of each predictor
require(caret)
flds <- createFolds(data$V9, k = 10, list = TRUE, returnTrain = FALSE)
for (i in 1:10){
testData <- data[flds[[i]], ]
trainData <- data[-flds[[i]],]
fit <- randomForest(V9 ~ .,   data=trainData)
pre <- predict(fit,testData)
print(confusionMatrix(pre,testData$V9))
}
setwd("/Users/taomo/Documents/workspace/MLProject/MachineLearningProject/Rcode")
library(rpart)
data = read.table('data.txt')
fit <- rpart(V9 ~ .,method="class",data=data)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
require(caret)
flds <- createFolds(data$V9, k = 10, list = TRUE, returnTrain = FALSE)
for (i in 1:10){
testData <- data[flds[[i]], ]
trainData <- data[-flds[[i]],]
fit <- rpart(V9 ~ .,   method="class", data=trainData)
pre <- predict(fit,testData,type="class")
print(confusionMatrix(pre,testData$V9))
}
plot(fit, uniform=TRUE,
main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
setwd("/Users/taomo/Documents/workspace/MLProject/MachineLearningProject/Rcode")
library(rpart)
data = read.table('data.txt')
fit <- rpart(V9 ~ .,method="class",data=data)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
require(caret)
flds <- createFolds(data$V9, k = 10, list = TRUE, returnTrain = FALSE)
for (i in 1:10){
testData <- data[flds[[i]], ]
trainData <- data[-flds[[i]],]
fit <- rpart(V9 ~ .,   method="class", data=trainData)
pre <- predict(fit,testData,type="class")
print(confusionMatrix(pre,testData$V9))
}
setwd("/Users/taomo/Documents/workspace/MLProject/MachineLearningProject/Rcode")
data = read.table('data.txt')
dataNum <- matrix(data = NA, nrow = dim(data)[1], ncol = dim(data)[2])
for (i in 1:dim(data)[2]) {
dataNum[,i] <- c(as.numeric(data[[i]]))-1}
library(e1071)
model <- svm(V9 ~ ., data = dataNum)
summary(model)
data = read.table('data.txt')
library(randomForest)
fit <- randomForest(V9 ~ .,   data=data)
print(fit) # view results
importance(fit) # importance of each predictor
require(caret)
flds <- createFolds(data$V9, k = 10, list = TRUE, returnTrain = FALSE)
for (i in 1:10){
testData <- data[flds[[i]], ]
trainData <- data[-flds[[i]],]
fit <- randomForest(V9 ~ .,   data=trainData)
pre <- predict(fit,testData)
print(confusionMatrix(pre,testData$V9))
}
install.packages("RSNNS")
library(RSNNS)
data(iris)
iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]
irisValues <- iris[,1:4]
irisTargets <- decodeClassLabels(iris[,5])
iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)
iris <- normTrainingAndTestSet(iris)
model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),
maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)
summary(model)
model
weightMatrix(model)
extractNetInfo(model)
par(mfrow=c(2,2))
plotIterativeError(model)
predictions <- predict(model,iris$inputsTest)
plotRegressionError(predictions[,2], iris$targetsTest[,2])
confusionMatrix(iris$targetsTrain,fitted.values(model))
confusionMatrix(iris$targetsTest,predictions)
plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])
plotROC(predictions[,2], iris$targetsTest[,2])
confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),
method="402040", l=0.4, h=0.6))
data = read.table('data.txt')
View(data)
dataValues <- data[,1:9]
dataTargets <- decodeClassLabels(data[,10])
View(dataTargets)
dataTargets <- decodeClassLabels(iris[,5])
View(dataTargets)
data
dataTargets <- decodeClassLabels(data[,10])
View(irisValues)
View(irisTargets)
View(dataTargets)
dataSplit <- splitForTrainingAndTest(dataValues, dataTargets, ratio=0.15)
dataSplit <- normTrainingAndTestSet(dataSplit)
model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),
maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)
summary(model)
model
weightMatrix(model)
extractNetInfo(model)
par(mfrow=c(2,2))
plotIterativeError(model)
predictions <- predict(model,iris$inputsTest)
plotRegressionError(predictions[,2], iris$targetsTest[,2])
confusionMatrix(iris$targetsTrain,fitted.values(model))
confusionMatrix(iris$targetsTest,predictions)
plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])
plotROC(predictions[,2], iris$targetsTest[,2])
confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),
method="402040", l=0.4, h=0.6))
View(predictions)
clean()
rm(list=ls(all=TRUE))
library(RSNNS)
setwd("/Users/taomo/Documents/workspace/MLProject/MachineLearningProject/Rcode")
data = read.table('data.txt')
iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]
dataValues <- data[,1:9]
dataTargets <- decodeClassLabels(data[,10])
dataSplit <- splitForTrainingAndTest(dataValues, dataTargets, ratio=0.15)
model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),
maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)
summary(model)
dataSplit <- spli tForTrainingAndTest(dataValues, dataTargets, ratio=0.15)
dataSplit <- splitForTrainingAndTest(dataValues, dataTargets, ratio=0.15)
model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),
maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)
model <- mlp(dataSplit$inputsTrain, dataSplit$targetsTrain, size=5, learnFuncParams=c(0.1),
maxit=50, inputsTest=dataSplit$inputsTest, targetsTest=dataSplit$targetsTest)
dataNum <- matrix(data = NA, nrow = dim(data)[1], ncol = dim(data)[2])
dataNum <- matrix(data = NA, nrow = dim(data)[1], ncol = dim(data)[2])
dataNum <- matrix(data = NA, nrow = dim(data)[1], ncol = dim(data)[2])
dataNum <- matrix(data = NA, nrow = dim(data)[1], ncol = dim(data)[2])
for (i in 1:dim(data)[2]) {
dataNum[,i] <- c(as.numeric(data[[i]]))-1}
dataValues <- dataNum[,1:9]
dataTargets <- decodeClassLabels(dataNum[,10])
dataSplit <- splitForTrainingAndTest(dataValues, dataTargets, ratio=0.15)
model <- mlp(dataSplit$inputsTrain, dataSplit$targetsTrain, size=5, learnFuncParams=c(0.1),
maxit=50, inputsTest=dataSplit$inputsTest, targetsTest=dataSplit$targetsTest)
summary(model)
model
weightMatrix(model)
extractNetInfo(model)
par(mfrow=c(2,2))
plotIterativeError(model)
predictions <- predict(model,iris$inputsTest)
plotRegressionError(predictions[,2], iris$targetsTest[,2])
confusionMatrix(iris$targetsTrain,fitted.values(model))
confusionMatrix(iris$targetsTest,predictions)
plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])
plotROC(predictions[,2], iris$targetsTest[,2])
confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),
method="402040", l=0.4, h=0.6))
summary(model)
model
weightMatrix(model)
extractNetInfo(model)
par(mfrow=c(2,2))
plotIterativeError(model)
predictions <- predict(model,iris$inputsTest)
par(mfrow=c(2,2))
plotIterativeError(model)
predictions <- predict(model,dataSplit$inputsTest)
plotRegressionError(predictions[,2], dataSplit$targetsTest[,2])
confusionMatrix(dataSplit$targetsTrain,fitted.values(model))
confusionMatrix(dataSplit$targetsTest,predictions)
plotROC(fitted.values(model)[,2], dataSplit$targetsTrain[,2])
plotROC(predictions[,2], dataSplit$targetsTest[,2])
confusionMatrix(dataSplit$targetsTrain, encodeClassLabels(fitted.values(model),
method="402040", l=0.4, h=0.6))
data(HouseVotes84)
model <- naiveBayes(Class ~ ., data = HouseVotes84)
library(e1071)
data(HouseVotes84)
model <- naiveBayes(Class ~ ., data = HouseVotes84)
data(HouseVotes84)
rm(list=ls(all=TRUE))
library(e1071)
data(HouseVotes84)
model <- naiveBayes(Class ~ ., data = HouseVotes84)
data(HouseVotes84)
data(HouseVotes84)
library(mlbench)
install.packages("mlbench")
data(HouseVotes84)
data("HouseVotes84")
data = read.table('data.txt')
View(data)
fit <- randomForest(V10 ~ .,   data=data)
print(fit) # view results
importance(fit) # importance of each predictor
require(caret)
flds <- createFolds(data$V9, k = 10, list = TRUE, returnTrain = FALSE)
for (i in 1:10){
testData <- data[flds[[i]], ]
trainData <- data[-flds[[i]],]
fit <- randomForest(V9 ~ .,   data=trainData)
pre <- predict(fit,testData)
print(confusionMatrix(pre,testData$V9))
}
model <- naiveBayes(V10 ~ .,   data=data)
predict(model, HouseVotes84[1:10,-1])
pred <- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$V10)
model <- naiveBayes(V10 ~ .,   data=data)
pred <- predict(model, data[,-1])
table(pred, data$V10)
require(caret)
flds <- createFolds(data$V10, k = 10, list = TRUE, returnTrain = FALSE)
for (i in 1:10){
testData <- data[flds[[i]], ]
trainData <- data[-flds[[i]],]
fit <- naiveBayes(V10 ~ .,   data=trainData)
pre <- predict(fit,testData)
print(confusionMatrix(pre,testData$V10))
}
